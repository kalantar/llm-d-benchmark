apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: experiment
spec:
  description: >
    Runs an llm-d-benchmark experiment.

  workspaces:
    - name: data

  params:
    - name: question_len
      type: string
    - name: output_len
      type: string
    - name: gaiePluginConfig
      type: string

    - name: targetNamespacePrefix
      type: string
      default: llmdbench

    - name: model-id
      type: string
      default: "meta-llama/Llama-3.2-1B-Instruct"
    - name: inferencePort
      default: 8000

    - name: experimentBaseUrl
      type: string
    - name: experimentName
      type: string
      default: "experiment"

    - name: model-pvc-name
      type: string
      default: model-pvc
    - name: model-pvc-size
      type: string
      default: 300Gi
    - name: model-storage-class
      type: string
      default: ocs-storagecluster-cephfs

    - name: download-job-name
      type: string
      default: download-job

    - default: llm-d-infra
      description: Name of the Helm repository for the Gateway
      name: gatewayRepoName
      type: string
    - default: https://llm-d-incubation.github.io/llm-d-infra/
      description: URL of the Helm repository for the Gateway
      name: gatewayRepoUrl
      type: string
    - name: gatewayChartVersion
      type: string
      default: ""
      description: Optional gateway chart version (used with --version)

    - name: gatewayExtraArgs
      type: string
      default: ""
      description: Optional extra args for the gateway (to append to 'helm upgrade --install')

    - name: gaieChartVersion
      type: string
      default: "v0.5.1"
      description: Optional GAIE chart version (used with --version)

    - name: gaieExtraArgs
      type: string
      default: ""
      description: Optional extra args for GAIE (to append to 'helm upgrade --install')

    - default: llm-d-modelservice
      description: Name of the Helm repository for the model engine
      name: msRepoName
      type: string
    - default: https://llm-d-incubation.github.io/llm-d-modelservice/
      description: URL of the Helm repository for the model engine
      name: msRepoUrl
      type: string
    - name: msChartVersion
      type: string
      default: ""
      description: Optional modelservice chart version (used with --version)

    - name: msExtraArgs
      type: string
      default: ""
      description: Optional extra args for the model engine (to append to 'helm upgrade --install')

    - name: modelWaitTimeout
      type: string
      default: 900

    - name: llmdbenchImageRegistry
      default: "quay.io"
    - name: llmdbenchImageRepo
      default: "namasluk"
    - name: llmdbenchImageName
      default: "llm-d-benchmark"
    - name: llmdbenchImageTag
      default: "251002.1"

    - name: harnessName
      type: string
      default: inference-perf
    - name: harnessProfile
      type: string
      default: sanity_random.yaml
    - name: stackType
      type: string
      default: lld-d
    - name: pipelineUID
      type: string
      default: experiment

    - name: s3-keys
      type: string
      default: "s3-keys"
    - name: s3-bucket
      type: string
    - name: s3-endpoint
      type: string

    - name: debug
      type: string
      default: "false"
    - name: dry-run
      type: string 
      default: "false"

  steps:
    - name: log-start
      image: alpine:3.20
      script: |
        #!/bin/sh
        echo "üîÑ Starting sweep step for ..."
        echo "     gaiePluginConfig = $(params.gaiePluginConfig)"
        echo "         question_len = $(params.question_len)"
        echo "           output_len = $(params.output_len)"

    - name: prepare-namespace
      image: quay.io/openshift/origin-cli:4.21
      script: |
        #!/bin/sh

        NAMESPACE="$(params.targetNamespacePrefix)-$(context.taskRun.name)"
        DRY_RUN="$(params.dry-run)"

        if [ "${DRY_RUN}" = "true" ]; then
          echo ">> skipping"
          exit 0
        fi

        kubectl create namespace ${NAMESPACE} \
          --dry-run=client -o yaml | kubectl apply -f -
        
        HF_TOKEN=$(
          kubectl get secret hf-secret \
          --namespace "$(context.taskRun.namespace)" \
            -o jsonpath='{.data.HF_TOKEN}' \
          | tr -d '\n' \
          | base64 -d
        )

        kubectl create secret generic hf-secret \
          --namespace ${NAMESPACE} \
          --from-literal="HF_TOKEN=${HF_TOKEN}" \
          --dry-run=client -o yaml | kubectl apply -f -

        # TBD only if OpenShift
        oc adm policy add-scc-to-user anyuid -z helm-installer -n ${NAMESPACE}
        # oc adm policy add-scc-to-user privileged -z helm-installer -n ${NAMESPACE}

    - name: model-download
      ref: 
        name: helm-upgrade-install
      params:
        # Location of helm chart
        - name: git_url
          value: "https://github.com/kalantar/llm-d-benchmark"
        - name: git_revision
          value: "tekton-poc"
        - name: checkout_dir
          value: "/tmp/llm-d-benchmark"

        # Helm arguments
        - name: releaseName
          value: $(params.experimentName)-download
        - name: chart
          value: /tmp/llm-d-benchmark/charts/model-download        
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: timeout
          value: 15m
        # - name: valuesYamlUrl
        #   value: "/tmp/llm-d-benchmark/charts/model-download/values.yaml"
        - name: extraArgs
          value: >
            --set hf_model=$(params.model-id) 
            --set pvc.create=true 
            --set pvc.name=$(params.model-pvc-name) 
            --set pvc.size=$(params.model-pvc-size) 
            --set pvc.storageClass=$(params.model-storage-class)

        - name: dry-run
          value: $(params.dry-run)

    - name: wait-for-download
      image: alpine:3.20
      script : |
        #!/bin/sh
        echo "‚è≥ TBD: Wait for download job to complete"

    - name: gateway
      ref: 
        name: helm-upgrade-install
      params:
        - name: releaseName
          value: $(params.experimentName)-gateway
        - name: chart
          value: llm-d-infra/llm-d-infra
        - name: repoName
          value: llm-d-infra
        - name: repoUrl
          value: https://llm-d-incubation.github.io/llm-d-infra/
        
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: timeout
          value: 15m
        - name: valuesYamlUrl
          value: "$(params.experimentBaseUrl)/gateway-values.yaml"

        - name: dry-run
          value: $(params.dry-run)
      
    - name: gaie
      ref: 
        name: helm-upgrade-install
      params:
        - name: releaseName
          value: $(params.experimentName)-gaie-NAMESPACE_HASH
        - name: chart
          value: oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
        - name: version
          value: $(params.gaieChartVersion)
        
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: timeout
          value: 15m
        - name: valuesYamlUrl
          value: "$(params.experimentBaseUrl)/gaie-values.yaml"
        - name: extraArgs
          value: "--set inferenceExtension.pluginsConfigFile=$(params.gaiePluginConfig)"

        - name: dry-run
          value: $(params.dry-run)

    - name: model-engine
      ref: 
        name: helm-upgrade-install
      params:
        - name: releaseName
          value: $(params.experimentName)-ms
        - name: chart
          value: llm-d-modelservice/llm-d-modelservice
        - name: repoName
          value: llm-d-modelservice
        - name: repoUrl
          value: https://llm-d-incubation.github.io/llm-d-modelservice/
        
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: timeout
          value: 15m
        - name: valuesYamlUrl
          value: "$(params.experimentBaseUrl)/ms-values.yaml"
        - name: extraArgs
          value: >
            --set routing.inferencePool.name=$(params.experimentName)-gaie-NAMESPACE_HASH
            --set routing.httpRoute.rules[0].backendRefs[0].name=$(params.experimentName)-gaie-NAMESPACE_HASH
            --set routing.httpRoute.rules[1].backendRefs[0].name=$(params.experimentName)-gaie-NAMESPACE_HASH

        - name: dry-run
          value: $(params.dry-run)

    - name: wait-for-model
      image: alpine/kubectl:1.34.1
      script: |
        #!/bin/sh
        
        if [ "$(params.dry-run)" = "true" ]; then
          echo ">> skipping"
          exit 0
        fi
        NAMESPACE="$(params.targetNamespacePrefix)-$(context.taskRun.name)"
        MODEL_ID="$(params.model-id)"
        MODEL_LABEL=$(echo "$MODEL_ID" | tr '[:upper:]' '[:lower:]' | sed 's/[./]/-/g')
        MODEL_START_TIMEOUT="$(params.modelWaitTimeout)"

        echo "‚è≥ Waiting for pods serving model ${MODEL_ID} to be 'Running'"
        echo "Model label = ${MODEL_LABEL}"

        kubectl --namespace ${NAMESPACE} \
          wait pod \
          -l llm-d.ai/model=${MODEL_LABEL},llm-d.ai/role=decode \
          --for=create \
          --timeout=${MODEL_START_TIMEOUT}s
        echo "‚úÖ (decode) pods serving model ${MODEL_ID} created"
 
        # TBD check if any prefill pods and wait if so
        # kubectl --namespace ${NAMESPACE} \
        #   wait pod \
        #   -l llm-d.ai/model=${MODEL_LABEL},llm-d.ai/role=prefill \
        #   --for=create \
        #   --timeout=${MODEL_START_TIMEOUT}s
        # echo "‚úÖ prefill pods serving model ${MODEL_ID} created"

        kubectl --namespace ${NAMESPACE} \
          wait pod \
          -l llm-d.ai/model=${MODEL_LABEL},llm-d.ai/role=decode \
          --for=condition=Ready=True \
          --timeout=${MODEL_START_TIMEOUT}s
        echo "‚úÖ (decode) pods serving model ${MODEL_ID} ready"

        # TBD check if any prefill pods and wait if so
        # kubectl --namespace ${NAMESPACE} \
        #   wait pod \
        #   -l llm-d.ai/model=${MODEL_LABEL},llm-d.ai/role=prefill \
        #   --for=condition=Ready=True \
        #   --timeout=${MODEL_START_TIMEOUT}s
        # echo "‚úÖ prefill pods serving model ${MODEL_ID} ready"

    - name: workload
      image: $(params.llmdbenchImageRegistry)/$(params.llmdbenchImageRepo)/$(params.llmdbenchImageName):$(params.llmdbenchImageTag)
      env:
        - name: LLMDBENCH_RUN_EXPERIMENT_LAUNCHER
          value: "1"
        - name: LLMDBENCH_RUN_EXPERIMENT_ANALYZE_LOCALLY
          value: "0"
        - name: LLMDBENCH_RUN_EXPERIMENT_HARNESS
          value: "$(params.harnessName)-llm-d-benchmark.sh"
        - name: LLMDBENCH_RUN_EXPERIMENT_ANALYZER
          value: "$(params.harnessName)-analyze_results.sh"
        - name: LLMDBENCH_RUN_EXPERIMENT_HARNESS_WORKLOAD_NAME
          value: "$(params.harnessProfile)"
        - name: LLMDBENCH_HARNESS_NAME
          value: "$(params.harnessName)"
        - name: LLMDBENCH_HARNESS_NAMESPACE
          value: "$(params.targetNamespacePrefix)-$(context.taskRun.name)"
        - name: LLMDBENCH_HARNESS_STACK_TYPE
          value: "llm-d"
        - name: LLMDBENCH_HARNESS_STACK_ENDPOINT_URL
          value: "http://experiment-gateway-inference-gateway.$(params.targetNamespacePrefix)-$(context.taskRun.name).svc.cluster.local:80"
        - name: LLMDBENCH_DEPLOY_METHODS
          value: "modelservice"
        - name: LLMDBENCH_MAGIC_ENVAR
          value: "harness_pod"

        - name: LLMDBENCH_LLMD_IMAGE_REGISTRY
          value: "$(params.llmdbenchImageRegistry)"
        - name: LLMDBENCH_LLMD_IMAGE_REPO
          value: "$(params.llmdbenchImageRepo)"
        - name: LLMDBENCH_LLMD_IMAGE_NAME
          value: "$(params.llmdbenchImageName)"
        - name: LLMDBENCH_LLMD_IMAGE_TAG
          value: "$(params.llmdbenchImageTag)"

        #  TBD add_env_vars_to_pod $LLMDBENCH_CONTROL_ENV_VAR_LIST_TO_POD
        - name: LLMDBENCH_DEPLOY_CURRENT_MODEL
          value: "$(params.model-id)"
        - name: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS
          value: "0"
        - name: LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS
          value: "2"
        - name: LLMDBENCH_VLLM_COMMON_AFFINITY
          value: "nvidia.com/gpu.product:NVIDIA-H100-80GB-HBM3"
        - name: LLMDBENCH_VLLM_MODELSERVICE_DECODE_TENSOR_PARALLELISM
          value: "4"
        - name: LLMDBENCH_VLLM_MODELSERVICE_DECODE_DATA_PARALLELISM
          value: "1"
        - name: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_TENSOR_PARALLELISM
          value: "1"
        - name: LLMDBENCH_VLLM_MODELSERVICE_PREFILL_DATA_PARALLELISM
          value: "1"

        - name: HF_TOKEN_SECRET
          value: "hf-secret"
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-secret
              key: HF_TOKEN

      computeResources:
        requests:
          memory: "32Gi"
          cpu: "16"
        limits:
          memory: "32Gi"
          cpu: "16"

      script: |
        #!/bin/bash

        export EXPERIMENT_ID="experiment-$(echo -n $(params.pipelineUID) | cut -c1-8)"
        export LLMDBENCH_RUN_EXPERIMENT_ID="${EXPERIMENT_ID}"
        export LLMDBENCH_RUN_EXPERIMENT_RESULTS_DIR="$(workspaces.data.path)/$(params.harnessName)_${EXPERIMENT_ID}_$(context.taskRun.name)"
        export LLMDBENCH_CONTROL_WORK_DIR="$(workspaces.data.path)/$(params.harnessName)_${EXPERIMENT_ID}_$(context.taskRun.name)"
        export LLMDBENCH_HARNESS_STACK_NAME=$(echo "$(params.model-id)" | tr '[:upper:]' '[:lower:]' | sed 's/[./]/-/g')
        export LLMDBENCH_DEPLOY_CURRENT_MODELID="${LLMDBENCH_HARNESS_STACK_NAME}"
        export LLMDBENCH_DEPLOY_CURRENT_TOKENIZER="$(params.model-id)"

        export QUESTION_LEN=$(params.question_len)
        export OUTPUT_LEN=$(params.output_len)

        get_profiles() {
          git init llm-d-benchmark
          cd llm-d-benchmark
          git remote add origin https://github.com/llm-d/llm-d-benchmark.git
          git config core.sparseCheckout true
          echo "workload/profiles/" >> .git/info/sparse-checkout
          git pull origin main
        }

        if [ "$(params.dry-run)" = "true" ]; then
          echo ">> skipping"
          exit 0
        fi

        get_profiles

        echo "creating CONTROL directories"
        mkdir -p ${LLMDBENCH_CONTROL_WORK_DIR}/setup
        rm -f ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands
        touch ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands

        workload=$(echo $(params.harnessProfile) | sed 's^\.yaml^^g' )
        echo "workload = $workload"
        workload_template_list=$(find workload/profiles/ -name "${workload}.yaml.in")
        echo "workload_template_list = $workload_template_list"

        for workload_template_full_path in $workload_template_list; do
          echo "PROCESSING $workload_template_full_path"
          workload_template_type=$(echo ${workload_template_full_path} | rev | cut -d '/' -f 2 | rev)
          echo "workload_template_type = $workload_template_type"
          workload_template_file_name=$(echo ${workload_template_full_path} | rev | cut -d '/' -f 1 | rev | sed -e "s^\.yaml.in$^^g")
          echo "workload_template_file_name = $workload_template_file_name"
          ## 
          workload_output_file=${LLMDBENCH_CONTROL_WORK_DIR}/${workload_template_file_name}.yaml
          # workload_output_file=${LLMDBENCH_CONTROL_WORK_DIR}/$workload_template_type/$workload_template_file_name
          echo "workload_output_file = $workload_output_file"
          ##
          mkdir -p ${LLMDBENCH_CONTROL_WORK_DIR}/$workload_template_type

          echo "s^REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL^${LLMDBENCH_DEPLOY_CURRENT_MODEL}^g" >> ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands
          echo "s^REPLACE_ENV_LLMDBENCH_HARNESS_STACK_ENDPOINT_URL^${LLMDBENCH_HARNESS_STACK_ENDPOINT_URL}^g" >> ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands
          echo "s^REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_TOKENIZER^${LLMDBENCH_DEPLOY_CURRENT_TOKENIZER}^g" >> ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands
          echo "s^question_len: .*^question_len: ${QUESTION_LEN}^g" >> ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands
          echo "s^output_len: .*^output_len: ${OUTPUT_LEN}^g" >> ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands
          echo "s^    path: .*^    path: ${LLMDBENCH_RUN_EXPERIMENT_RESULTS_DIR}^g" >> ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands

          echo "------"
          cat ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands
          echo "------"
          echo "workload_output_file=$workload_output_file"
          sed -f ${LLMDBENCH_CONTROL_WORK_DIR}/setup/sed-commands $workload_template_full_path > $workload_output_file

          cat $workload_output_file
        done

        llm-d-benchmark.sh

    - name: upload-results
      image: ubuntu:24.04
      # Tried amazon/aws-cli:2.31.9 but latest tar available via `dnf install tar -y` is 1.34. 
      # There were sporadic errors "file changed as we read it". It may be caused by the way 
      # tar identifes file changes in v 1.34 (via ctime). A recommended solution to move to 1.35. 
      # See https://stackoverflow.com/a/77765876 and tar release notes https://lists.gnu.org/archive/html/info-gnu/2023-07/msg00005.html)
      # A smaller image is probably desirable. A restriction is that AWS CLI v2 requires glibc.
      workingDir: $(workspaces.data.path)
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: $(params.s3-keys)
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: $(params.s3-keys)
              key: AWS_SECRET_ACCESS_KEY
        - name: AWS_EC2_METADATA_DISABLED
          value: "true"
      script: |
        #!/usr/bin/env sh

        apt-get update && \
            apt-get install -y --no-install-recommends ca-certificates curl unzip tar gzip && \
            rm -rf /var/lib/apt/lists/*

        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o /tmp/awscliv2.zip && \
          unzip /tmp/awscliv2.zip -d /tmp && \
          /tmp/aws/install && \
        rm -rf /tmp/aws /tmp/awscliv2.zip

        tar --version && gzip --version && aws --version

        EXPERIMENT_ID="experiment-$(echo -n $(params.pipelineUID) | cut -c1-8)"
        EXPERIMENT_RESULTS_FOLDER="$(params.harnessName)_${EXPERIMENT_ID}_$(context.taskRun.name)"
        ARCHIVE_NAME="${EXPERIMENT_RESULTS_FOLDER}.tar.gz"

        tar --version && gzip --version && aws --version

        tar -czf ${ARCHIVE_NAME} \
            -C "$(workspaces.data.path)" ${EXPERIMENT_RESULTS_FOLDER}

        aws s3 cp ${ARCHIVE_NAME} "s3://$(params.s3-bucket)/${ARCHIVE_NAME}" \
            --endpoint-url "$(params.s3-endpoint)" \
            --content-type "application/x-tar" \
            --content-encoding "gzip" \
            --no-progress
            # --recursive \

        rm -rf ${ARCHIVE_NAME}
        
        echo "‚úÖ Uploaded results to ${ARCHIVE_NAME}"

    - name: delete-namespace
      image: alpine/kubectl:1.34.1
      script : |
        #!/bin/sh

        NAMESPACE="$(params.targetNamespacePrefix)-$(context.taskRun.name)"
        DEBUG="$(params.debug)"

        if [ "$(params.debug)" = "true" ]; then
          echo "‚ö†Ô∏è DEBUG=true; leaving namespace ${NAMESPACE} for inspection"
          echo "‚ö†Ô∏è Manually clean up resources with \"kubectl delete namespace ${NAMESPACE}\""
          exit 0
        fi

        kubectl delete namespace ${NAMESPACE}
        echo "‚úÖ workload namespace ${NAMESPACE} deleted"

    - name: log-completion
      image: alpine:3.20
      script: |
        #!/bin/sh
        echo "‚úÖ Sweep step complete."
