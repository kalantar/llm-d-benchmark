apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: experiment
spec:
  description: >
    Runs an llm-d-benchmark experiment.

  workspaces:
    - name: data

  params:
    - name: factorMapping
      type: string
      description: |
        JSON string mapping factor to path in source yaml file sorted by purpose.
    - name: treatment
      type: string
      description: |
        JSON string of factors and values for one treatment. 
        Includes both infrastructure and workload factors.

    - name: targetNamespacePrefix
      type: string
      default: llmdbench

    - name: model-id
      type: string
      default: "meta-llama/Llama-3.2-1B-Instruct"
    - name: inferencePort
      default: 8000

    - name: experimentBaseUrl
      type: string
    - name: experimentName
      type: string
      default: "experiment"

    - name: model-pvc-name
      type: string
      default: model-pvc
    - name: model-pvc-size
      type: string
      default: 300Gi
    - name: model-storage-class
      type: string
      default: ocs-storagecluster-cephfs

    - name: download-job-name
      type: string
      default: download-job

    - default: llm-d-infra
      description: Name of the Helm repository for the Gateway
      name: gatewayRepoName
      type: string
    - default: https://llm-d-incubation.github.io/llm-d-infra/
      description: URL of the Helm repository for the Gateway
      name: gatewayRepoUrl
      type: string
    - name: gatewayChartVersion
      type: string
      default: ""
      description: Optional gateway chart version (used with --version)

    - name: gatewayExtraArgs
      type: string
      default: ""
      description: Optional extra args for the gateway (to append to 'helm upgrade --install')

    - name: gaieChartVersion
      type: string
      default: "v0.5.1"
      description: Optional GAIE chart version (used with --version)

    - name: gaieExtraArgs
      type: string
      default: ""
      description: Optional extra args for GAIE (to append to 'helm upgrade --install')

    - default: llm-d-modelservice
      description: Name of the Helm repository for the model engine
      name: msRepoName
      type: string
    - default: https://llm-d-incubation.github.io/llm-d-modelservice/
      description: URL of the Helm repository for the model engine
      name: msRepoUrl
      type: string
    - name: msChartVersion
      type: string
      default: ""
      description: Optional modelservice chart version (used with --version)

    - name: msExtraArgs
      type: string
      default: ""
      description: Optional extra args for the model engine (to append to 'helm upgrade --install')

    - name: modelWaitTimeout
      type: string
      default: 900

    - name: llmdbenchImageRegistry
      default: "quay.io"
    - name: llmdbenchImageRepo
      default: "namasluk"
    - name: llmdbenchImageName
      default: "llm-d-benchmark"
    - name: llmdbenchImageTag
      default: "251002.1"

    - name: harnessName
      type: string
      default: inference-perf
    - name: harnessProfile
      type: string
      default: sanity_random.yaml
    - name: stackType
      type: string
      default: lld-d
    - name: pipelineUID
      type: string
      default: experiment

    - name: s3-keys
      type: string
      default: "s3-keys"
    - name: s3-bucket
      type: string
    - name: s3-endpoint
      type: string

    - name: debug
      type: string
      default: "false"
    - name: dry-run
      type: string 
      default: "false"

  results:
    - name: treatmentAnalysisModelservice
      value: $(steps.analyze-modelservice-factors.results.treatmentAnalysis)
    - name: treatmentAnalysisGaie
      value: $(steps.analyze-gaie-factors.results.treatmentAnalysis)
    - name: treatmentAnalysisWorkload
      value: $(steps.analyze-workload-factors.results.treatmentAnalysis)

  steps:
    - name: log-start
      image: alpine:3.20
      script: |
        #!/bin/sh
        echo "üîÑ Starting sweep step for ..."
        printf "%s" "$(params.treatment)"

    - name: analyze-modelservice-factors
      ref:
        name: analyze-treatment
      params:
        - name: factorType
          value: modelservice
        - name: factorMapping
          value: $(params.factorMapping)
        - name: treatment
          value: $(params.treatment)

    - name: analyze-gaie-factors
      ref:
        name: analyze-treatment
      params:
        - name: factorType
          value: gaie
        - name: factorMapping
          value: $(params.factorMapping)
        - name: treatment
          value: $(params.treatment)

    - name: analyze-workload-factors
      ref:
        name: analyze-treatment
      params:
        - name: factorType
          value: workload
        - name: factorMapping
          value: $(params.factorMapping)
        - name: treatment
          value: $(params.treatment)

    - name: display-treatment-analysis
      image: alpine:3.20
      env:
        - name: MODELSERVICE_SET_ARGS
          value: "$(steps.analyze-modelservice-factors.results.treatmentAnalysis)"
        - name: GAIE_SET_ARGS
          value: "$(steps.analyze-gaie-factors.results.treatmentAnalysis)"
        - name: WORKLOAD_SET_ARGS
          value: "$(steps.analyze-workload-factors.results.treatmentAnalysis)"

      script: |
        #!/bin/sh
        apk add --no-cache jq yq-go >/dev/null
        jq --version

        echo "helm upgrade --install ... $(echo ${MODELSERVICE_SET_ARGS} | jq '.setArgs')"
        echo "helm upgrade --install ...  $(echo ${GAIE_SET_ARGS} | jq '.setArgs')"
        echo "$(echo ${WORKLOAD_SET_ARGS} | jq '.updates')"

    - name: prepare-namespace
      image: quay.io/openshift/origin-cli:4.21
      script: |
        #!/bin/sh

        NAMESPACE="$(params.targetNamespacePrefix)-$(context.taskRun.name)"
        DRY_RUN="$(params.dry-run)"

        if [ "${DRY_RUN}" = "true" ]; then
          echo ">> skipping"
          exit 0
        fi

        kubectl create namespace ${NAMESPACE} \
          --dry-run=client -o yaml | kubectl apply -f -
        
        HF_TOKEN=$(
          kubectl get secret hf-secret \
          --namespace "$(context.taskRun.namespace)" \
            -o jsonpath='{.data.HF_TOKEN}' \
          | tr -d '\n' \
          | base64 -d
        )

        kubectl create secret generic hf-secret \
          --namespace ${NAMESPACE} \
          --from-literal="HF_TOKEN=${HF_TOKEN}" \
          --dry-run=client -o yaml | kubectl apply -f -

        # TBD only if OpenShift
        oc adm policy add-scc-to-user anyuid -z helm-installer -n ${NAMESPACE}
        # oc adm policy add-scc-to-user privileged -z helm-installer -n ${NAMESPACE}

    - name: model-download
      ref: 
        name: helm-upgrade-install
      params:
        # Location of helm chart
        - name: git_url
          value: "https://github.com/kalantar/llm-d-benchmark"
        - name: git_revision
          value: "tekton-poc"
        - name: checkout_dir
          value: "/tmp/llm-d-benchmark"

        # Helm arguments
        - name: releaseName
          value: $(params.experimentName)-download
        - name: chart
          value: /tmp/llm-d-benchmark/charts/model-download        
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: timeout
          value: 15m
        # - name: valuesYamlUrl
        #   value: "/tmp/llm-d-benchmark/charts/model-download/values.yaml"
        - name: extraArgs
          value: >
            --set hf_model=$(params.model-id) 
            --set pvc.create=true 
            --set pvc.name=$(params.model-pvc-name) 
            --set pvc.size=$(params.model-pvc-size) 
            --set pvc.storageClass=$(params.model-storage-class)

        - name: dry-run
          value: $(params.dry-run)

    - name: wait-for-download
      image: alpine:3.20
      script : |
        #!/bin/sh
        echo "‚è≥ TBD: Wait for download job to complete"

    - name: gateway
      ref: 
        name: helm-upgrade-install
      params:
        - name: releaseName
          value: $(params.experimentName)-gateway
        - name: chart
          value: llm-d-infra/llm-d-infra
        - name: repoName
          value: llm-d-infra
        - name: repoUrl
          value: https://llm-d-incubation.github.io/llm-d-infra/
        
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: timeout
          value: 15m
        - name: valuesYamlUrl
          value: "$(params.experimentBaseUrl)/gateway-values.yaml"

        - name: dry-run
          value: $(params.dry-run)
      
    - name: gaie
      ref: 
        name: helm-upgrade-install
      params:
        - name: releaseName
          value: $(params.experimentName)-gaie-NAMESPACE_HASH
        - name: chart
          value: oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
        - name: version
          value: $(params.gaieChartVersion)
        
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: timeout
          value: 15m
        - name: valuesYamlUrl
          value: "$(params.experimentBaseUrl)/gaie-values.yaml"
        - name: treatmentAnalysis
          value: "$(steps.analyze-gaie-factors.results.treatmentAnalysis)"

        - name: dry-run
          value: $(params.dry-run)

    - name: model-engine
      ref: 
        name: helm-upgrade-install
      params:
        - name: releaseName
          value: $(params.experimentName)-ms
        - name: chart
          value: llm-d-modelservice/llm-d-modelservice
        - name: repoName
          value: llm-d-modelservice
        - name: repoUrl
          value: https://llm-d-incubation.github.io/llm-d-modelservice/
        
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: timeout
          value: 15m
        - name: valuesYamlUrl
          value: "$(params.experimentBaseUrl)/ms-values.yaml"
        - name: extraArgs
          value: >
            --set routing.inferencePool.name=$(params.experimentName)-gaie-NAMESPACE_HASH
            --set routing.httpRoute.rules[0].backendRefs[0].name=$(params.experimentName)-gaie-NAMESPACE_HASH
            --set routing.httpRoute.rules[1].backendRefs[0].name=$(params.experimentName)-gaie-NAMESPACE_HASH
        - name: treatmentAnalysis
          value: "$(steps.analyze-modelservice-factors.results.treatmentAnalysis)"

        - name: dry-run
          value: $(params.dry-run)

    - name: wait-for-model
      image: alpine/kubectl:1.34.1
      script: |
        #!/bin/sh
        
        if [ "$(params.dry-run)" = "true" ]; then
          echo ">> skipping"
          exit 0
        fi
        NAMESPACE="$(params.targetNamespacePrefix)-$(context.taskRun.name)"
        MODEL_ID="$(params.model-id)"
        MODEL_LABEL=$(echo "$MODEL_ID" | tr '[:upper:]' '[:lower:]' | sed 's/[./]/-/g')
        MODEL_START_TIMEOUT="$(params.modelWaitTimeout)"

        echo "‚è≥ Waiting for pods serving model ${MODEL_ID} to be 'Running'"
        echo "Model label = ${MODEL_LABEL}"

        kubectl --namespace ${NAMESPACE} \
          wait pod \
          -l llm-d.ai/model=${MODEL_LABEL},llm-d.ai/role=decode \
          --for=create \
          --timeout=${MODEL_START_TIMEOUT}s
        echo "‚úÖ (decode) pods serving model ${MODEL_ID} created"
 
        # TBD check if any prefill pods and wait if so
        # kubectl --namespace ${NAMESPACE} \
        #   wait pod \
        #   -l llm-d.ai/model=${MODEL_LABEL},llm-d.ai/role=prefill \
        #   --for=create \
        #   --timeout=${MODEL_START_TIMEOUT}s
        # echo "‚úÖ prefill pods serving model ${MODEL_ID} created"

        kubectl --namespace ${NAMESPACE} \
          wait pod \
          -l llm-d.ai/model=${MODEL_LABEL},llm-d.ai/role=decode \
          --for=condition=Ready=True \
          --timeout=${MODEL_START_TIMEOUT}s
        echo "‚úÖ (decode) pods serving model ${MODEL_ID} ready"

        # TBD check if any prefill pods and wait if so
        # kubectl --namespace ${NAMESPACE} \
        #   wait pod \
        #   -l llm-d.ai/model=${MODEL_LABEL},llm-d.ai/role=prefill \
        #   --for=condition=Ready=True \
        #   --timeout=${MODEL_START_TIMEOUT}s
        # echo "‚úÖ prefill pods serving model ${MODEL_ID} ready"

    - name: inference-perf-prepare-profile
      ref: 
        name: inference-perf-prepare-profile
      params:
        - name: harnessName
          value: $(params.harnessName)
        - name: harnessProfile
          value: $(params.harnessProfile)
        - name: treatmentAnalysis
          value: $(steps.analyze-workload-factors.results.treatmentAnalysis)
        - name: model-id
          value: $(params.model-id)
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: pipelineUID
          value: $(params.pipelineUID)

    - name: inference-perf-run
      ref:
        name: inference-perf-run
      params:
        - name: harnessName
          value: $(params.harnessName)
        - name: harnessProfile
          value: $(params.harnessProfile)
        - name: pipelineUID
          value: $(params.pipelineUID)
      computeResources:
        requests:
          memory: "32Gi"
          cpu: "16"
        limits:
          memory: "32Gi"
          cpu: "16"

    - name: inference-perf-analyze-results
      ref:
        name: inference-perf-analyze-results
      params:
        - name: harnessName
          value: $(params.harnessName)
        - name: pipelineUID
          value: $(params.pipelineUID)

    - name: vllm-benchmark-prepare-profile
      ref: 
        name: vllm-benchmark-prepare-profile
      params:
        - name: harnessName
          value: $(params.harnessName)
        - name: harnessProfile
          value: $(params.harnessProfile)
        - name: treatmentAnalysis
          value: $(steps.analyze-workload-factors.results.treatmentAnalysis)
        - name: model-id
          value: $(params.model-id)
        - name: namespace
          value: $(params.targetNamespacePrefix)-$(context.taskRun.name)
        - name: pipelineUID
          value: $(params.pipelineUID)

    - name: vllm-benchmark-run
      ref:
        name: vllm-benchmark-run
      params:
        - name: harnessName
          value: $(params.harnessName)
        - name: harnessProfile
          value: $(params.harnessProfile)
        - name: pipelineUID
          value: $(params.pipelineUID)
      computeResources:
        requests:
          memory: "32Gi"
          cpu: "16"
        limits:
          memory: "32Gi"
          cpu: "16"

    - name: vllm-benchmark-analyze-results
      ref:
        name: vllm-benchmark-analyze-results
      params:
        - name: harnessName
          value: $(params.harnessName)
        - name: pipelineUID
          value: $(params.pipelineUID)

    - name: delete-namespace
      image: alpine/kubectl:1.34.1
      script : |
        #!/bin/sh

        NAMESPACE="$(params.targetNamespacePrefix)-$(context.taskRun.name)"
        DEBUG="$(params.debug)"

        if [ "$(params.debug)" = "true" ]; then
          echo "‚ö†Ô∏è DEBUG=true; leaving namespace ${NAMESPACE} for inspection"
          echo "‚ö†Ô∏è Manually clean up resources with \"kubectl delete namespace ${NAMESPACE}\""
          exit 0
        fi

        kubectl delete namespace ${NAMESPACE}
        echo "‚úÖ workload namespace ${NAMESPACE} deleted"

    - name: log-completion
      image: alpine:3.20
      script: |
        #!/bin/sh
        echo "‚úÖ Sweep step complete."
