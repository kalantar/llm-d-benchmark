apiVersion: tekton.dev/v1beta1
kind: StepAction
metadata:
  name: vllm-benchmark-prepare-profile
spec:
  params:
    - name: harnessName
    - name: harnessProfile
    - name: model-id
    - name: namespace
    - name: treatmentAnalysis
    - name: pipelineUID
  env:
    - name: REQUESTED_HARNESS_NAME
      value: "$(params.harnessName)"
    - name: MY_HARNESS_NAME
      value: "vllm-benchmark"
    - name: HARNESS_PROFILE
      value: "$(params.harnessProfile)"

    - name: TREATMENT_ANALYSIS
      value: "$(params.treatmentAnalysis)"

    - name: LLMDBENCH_DEPLOY_CURRENT_MODEL
      value: "$(params.model-id)"
    - name: LLMDBENCH_HARNESS_STACK_ENDPOINT_URL
      value: "http://experiment-gateway-inference-gateway.$(params.namespace).svc.cluster.local:80"

    - name: DATA_ROOT_DIR
      value: $(workspaces.data.path)
    - name: MY_TASK_NAME
      value: $(context.taskRun.name)
    - name: MY_PIPELINE_UID
      value: $(params.pipelineUID)

  # https://github.com/llm-d/llm-d-benchmark/blob/main/build/Dockerfile#L1C6-L1C33
  image: python:3.12.9-slim-bookworm
  script: |
    #!/bin/bash

    if [ "${REQUESTED_HARNESS_NAME}" != "${MY_HARNESS_NAME}" ]; then
      echo "Requested harness not ${MY_HARNESS_NAME}, skipping"
      exit 0
    fi

    # TBD is this necessary or is it already there?
    apt-get update
    apt-get install -y --no-install-recommends curl ca-certificates jq
    curl -fsSL https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 \
      -o /usr/local/bin/yq
    chmod +x /usr/local/bin/yq
    jq --version
    yq --version

    # https://github.com/llm-d/llm-d-benchmark/blob/main/build/Dockerfile#L56-L62
    # https://github.com/llm-d/llm-d-benchmark/blob/main/setup/run.sh
    
    EXPERIMENT_ID="experiment-$(echo -n ${MY_PIPELINE_UID} | cut -c1-8)"
    RESULTS_DIR="${DATA_ROOT_DIR}/${MY_HARNESS_NAME}_${EXPERIMENT_ID}_${MY_TASK_NAME}"
    CONTROL_DIR="${DATA_ROOT_DIR}/${MY_HARNESS_NAME}_${EXPERIMENT_ID}_${MY_TASK_NAME}"
    RUN_DIR=$(pwd)

    echo "üîÑ Installing required tools"
    apt-get update
    apt-get install -y \
      wget \
      && apt-get clean && rm -rf /var/cache/apt

    # Ensure all folders created
    mkdir -p $RESULTS_DIR
    mkdir -p $CONTROL_DIR/setup
    rm -rf $CONTROL_DIR/setup/sed-commands
    touch $CONTROL_DIR/setup/sed-commands
    mkdir -p ${CONTROL_DIR}/workload/profiles/${MY_HARNESS_NAME}/templates

    cd ${RUN_DIR}/vllm-benchmark/

    # Define constants: input profile template name and location; final profile name and location
    workload=$(echo ${HARNESS_PROFILE} | sed 's^\.yaml^^g' )
    workload_template=${workload}.yaml.in
    workload_template_path=${CONTROL_DIR}/workload/profiles/${MY_HARNESS_NAME}/templates/${workload_template}
    workload_profile=${workload}.yaml
    workload_profile_path=${CONTROL_DIR}/workload/profiles/${MY_HARNESS_NAME}/${workload_profile}

    echo "üîÑ Prepare workload profile"
    # Fetch profile template from llmd-benchmark
    wget -O ${workload_template_path} \
    --quiet \
      https://raw.githubusercontent.com/llm-d/llm-d-benchmark/refs/heads/main/workload/profiles/${MY_HARNESS_NAME}/${workload_template}

    # Apply treatment to profile template to produce final profile
    echo "s^REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL^${LLMDBENCH_DEPLOY_CURRENT_MODEL}^g" >> ${CONTROL_DIR}/setup/sed-commands
    echo "s^REPLACE_ENV_LLMDBENCH_HARNESS_STACK_ENDPOINT_URL^${LLMDBENCH_HARNESS_STACK_ENDPOINT_URL}^g" >> ${CONTROL_DIR}/setup/sed-commands
    echo "s^REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_TOKENIZER^${LLMDBENCH_DEPLOY_CURRENT_TOKENIZER}^g" >> ${CONTROL_DIR}/setup/sed-commands
    echo "---------- sed-commands"
    cat ${CONTROL_DIR}/setup/sed-commands
    echo "----------"
    sed -f ${CONTROL_DIR}/setup/sed-commands ${workload_template_path} > ${workload_profile_path}

    # TBD eliminate the TARGET_FILE env variable
    TARGET_FILE=${workload_profile_path}
    echo "${TREATMENT_ANALYSIS}" | jq '.updates' > /tmp/updates.json
    echo ">>> /tmp/updates.json"
    cat /tmp/updates.json

    if [ ! -f "$TARGET_FILE" ]; then
      echo "ERROR: File not found: $TARGET_FILE" >&2
      exit 1
    fi

    # Apply updates to JSON or YAML
    if [ "$(jq 'length' /tmp/updates.json)" -gt 0 ]; then
      ext="${TARGET_FILE##*.}"
      tmp="${TARGET_FILE}.tmp"

      # TBD eliminate the json path (copilot generated this); profiles are yaml files
      if [ "$ext" = "json" ]; then
        jq --slurpfile upds /tmp/updates.json '
          reduce $upds[0][] as $u (. ; setpath($u.path; $u.value))
        ' "$TARGET_FILE" > "$tmp"
        mv "$tmp" "$TARGET_FILE"
      else
        # YAML path: YAML ‚Üí JSON ‚Üí apply ‚Üí YAML
        yq -o=json '.' "$TARGET_FILE" \
          | jq --slurpfile upds /tmp/updates.json '
              reduce $upds[0][] as $u (. ; setpath($u.path; $u.value))
            ' \
          | yq -P > "$tmp"
        mv "$tmp" "$TARGET_FILE"
      fi
    fi

    echo "---------- workload profile"
    cat ${workload_profile_path}
    echo "----------"
    echo "‚úÖ workload profile ready"
---
apiVersion: tekton.dev/v1beta1
kind: StepAction
metadata:
  name: vllm-benchmark-run
spec:
  params:
    - name: harnessName
    - name: harnessProfile
    - name: pipelineUID
  env:
    - name: REQUESTED_HARNESS_NAME
      value: "$(params.harnessName)"
    - name: MY_HARNESS_NAME
      value: "vllm-benchmark"
    - name: HARNESS_PROFILE
      value: "$(params.harnessProfile)"

    - name: GIT_REPO_URL
      value: "https://github.com/vllm-project/vllm.git"
    - name: GIT_REVISION
      value: "main"
    - name: GIT_COMMIT
      value: "b6381ced9c52271f799a8348fcc98c5f40528cdf"

    - name: DATA_ROOT_DIR
      value: $(workspaces.data.path)
    - name: MY_PIPELINE_UID
      value: $(params.pipelineUID)
    - name: MY_TASK_NAME
      value: $(context.taskRun.name)

    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-secret
          key: HF_TOKEN

  # https://github.com/llm-d/llm-d-benchmark/blob/main/build/Dockerfile#L1C6-L1C33
  image: python:3.12.9-slim-bookworm
  script: |
    #!/bin/bash

    # https://github.com/llm-d/llm-d-benchmark/blob/main/workload/harnesses/vllm-benchmark-llm-d-benchmark.sh

    if [ "${REQUESTED_HARNESS_NAME}" != "${MY_HARNESS_NAME}" ]; then
      echo "Requested harness not ${MY_HARNESS_NAME}, skipping"
      exit 0
    fi

    # https://github.com/llm-d/llm-d-benchmark/blob/main/build/Dockerfile#L56-L62
    # https://github.com/llm-d/llm-d-benchmark/blob/main/setup/run.sh

    EXPERIMENT_ID="experiment-$(echo -n ${MY_PIPELINE_UID} | cut -c1-8)"
    RESULTS_DIR="${DATA_ROOT_DIR}/${MY_HARNESS_NAME}_${EXPERIMENT_ID}_${MY_TASK_NAME}"
    CONTROL_DIR="${DATA_ROOT_DIR}/${MY_HARNESS_NAME}_${EXPERIMENT_ID}_${MY_TASK_NAME}"
    RUN_DIR=$(pwd)

    # TODO figure out which are actually needed for each step
    echo "üîÑ Installing required tools"
    apt-get update
    apt-get install -y \
      git \
      gpg \
      pip \
      yq \
      && apt-get clean && rm -rf /var/cache/apt

    echo "üîÑ Cloning and installing harness: ${MY_HARNESS_NAME}"
    git clone --branch ${GIT_REVISION} ${GIT_REPO_URL}
    cd vllm
    git checkout ${GIT_COMMIT}
    cd ..
    mv -f vllm vllm-benchmark

    # TBD pin versions
    cat <<EOF > requirements-vllm-benchmark.txt
    aiohttp
    datasets
    numpy
    pandas
    pillow
    tqdm
    transformers
    EOF

    cat requirements-vllm-benchmark.txt
    pip --version
    pip install --no-cache-dir \
      --disable-pip-version-check  \
      --upgrade \
      -r ./requirements-vllm-benchmark.txt \
      --root-user-action=ignore
    pip list

    # profile name and location
    workload=$(echo ${HARNESS_PROFILE} | sed 's^\.yaml^^g' )
    workload_profile=${workload}.yaml
    workload_profile_path=${CONTROL_DIR}/workload/profiles/${MY_HARNESS_NAME}/${workload_profile}

    # run vllm-benchmark
    cp ${workload_profile_path} ${workload_profile}
    en=$(cat ${workload_profile} | yq -r .executable)

    echo "pwd = $(pwd)"
    echo "RUN_DIR=$RUN_DIR"
    echo "running - ${RUN_DIR}/vllm-benchmark/benchmarks/${en}"
    ls -l ${RUN_DIR}/vllm-benchmark/benchmarks
    python ${RUN_DIR}/vllm-benchmark/benchmarks/${en} --$(cat ${workload_profile} | grep -v "^executable" | yq -r 'to_entries | map("\(.key)=\(.value)") | join(" --")' | sed -e 's^=none ^^g' -e 's^=none$^^g')  --seed $(date +%s) --save-result > >(tee -a $RESULTS_DIR/stdout.log) 2> >(tee -a $RESULTS_DIR/stderr.log >&2)
    export LLMDBENCH_RUN_EXPERIMENT_HARNESS_RC=$?
    find ${RUN_DIR}/vllm-benchmark -maxdepth 1 -mindepth 1 -name '*.json' -exec mv -t "$RESULTS_DIR"/ {} +

    # If benchmark harness returned with an error, exit here
    if [[ $LLMDBENCH_RUN_EXPERIMENT_HARNESS_RC -ne 0 ]]; then
      echo "‚ùå Harness returned with error $LLMDBENCH_RUN_EXPERIMENT_HARNESS_RC"
      exit $LLMDBENCH_RUN_EXPERIMENT_HARNESS_RC
    fi
    echo "‚úÖ Harness completed successfully."
---
apiVersion: tekton.dev/v1beta1
kind: StepAction
metadata:
  name: vllm-benchmark-analyze-results
spec:
  params:
    - name: harnessName
    - name: pipelineUID
  env:
    - name: REQUESTED_HARNESS_NAME
      value: "$(params.harnessName)"
    - name: MY_HARNESS_NAME
      value: "vllm-benchmark"

    - name: GIT_REPO_URL
      value: "https://github.com/kubernetes-sigs/inference-perf.git"
    - name: GIT_REVISION
      value: "main"
    - name: GIT_COMMIT
      value: "1ccc48b6bb9c9abb61558b719041fb000b265e59"

    - name: DATA_ROOT_DIR
      value: $(workspaces.data.path)
    - name: MY_PIPELINE_UID
      value: $(params.pipelineUID)
    - name: MY_TASK_NAME
      value: $(context.taskRun.name)

# https://github.com/llm-d/llm-d-benchmark/blob/main/build/Dockerfile#L1C6-L1C33
  image: python:3.12.9-slim-bookworm
  script: |
    #!/usr/bin/env bash

    EXPERIMENT_ID="experiment-$(echo -n ${MY_PIPELINE_UID} | cut -c1-8)"
    RESULTS_DIR="${DATA_ROOT_DIR}/${MY_HARNESS_NAME}_${EXPERIMENT_ID}_${MY_TASK_NAME}"
 
    if [ "${REQUESTED_HARNESS_NAME}" != "${MY_HARNESS_NAME}" ]; then
      echo "Requested harness not ${MY_HARNESS_NAME}, skipping"
      exit 0
    fi
     
    echo "üîÑ Installing requirements"
    # apt-get update
    # apt-get install -y \
    #   git \
    #   pip \
    #   && apt-get clean && rm -rf /var/cache/apt

    cat <<EOF > requirements-analysis.txt
    matplotlib>=3.7.0
    numpy>=2.3.1
    seaborn>=0.12.0
    pandas>=2.2.3
    pydantic>=2.11.7
    PyYAML>=6.0.2
    scipy>=1.16.0
    requests>=2.32.5
    EOF

    cat requirements-analysis.txt
    pip --version
    pip install --no-cache-dir \
      --disable-pip-version-check  \
      --upgrade \
      -r ./requirements-analysis.txt \
      --root-user-action=ignore
    pip list

    # Download covert python from llm-d-benchmark
    # TBD: should the python be embedded in the step? A separate step perhaps.
    export ROOT_DIR=workload/report
    export BRANCH=main

    cat <<EOF | python
    import os
    import requests

    # TBD these should be parameters
    ROOT_DIR = os.getenv("ROOT_DIR")
    BRANCH = os.getenv("BRANCH")

    api = f"https://api.github.com/repos/llm-d/llm-d-benchmark/contents/{ROOT_DIR}?ref={BRANCH}"
    headers = {}

    resp = requests.get(api, headers={})
    resp.raise_for_status()
    for item in resp.json():
        if item.get("type") == "file" and item["name"].endswith(".py"):
            url = item["download_url"]
            r = requests.get(url, headers=headers)
            r.raise_for_status()
            with open(item["name"], "wb") as f:
                f.write(r.content)
            print("Downloaded", item["name"])
    EOF

    chmod +x convert.py schema.py
    ls -l

    # https://github.com/llm-d/llm-d-benchmark/blob/main/workload/harnesses/inference-perf-llm-d-benchmark.sh#L17C1-L26C5
    echo "üîÑ Convert results into universal format"
    for result in $(find $RESULTS_DIR -maxdepth 1 -name 'stage_*.json'); do
      result_fname=$(echo $result | rev | cut -d '/' -f 1 | rev)
      ./convert.py $result -w inference-perf $RESULTS_DIR/benchmark_report,_$result_fname.yaml 2> >(tee -a $RESULTS_DIR/stderr.log >&2)
      # Report errors but don't quit
      export RUN_EXPERIMENT_CONVERT_RC=$?
      if [[ $RUN_EXPERIMENT_CONVERT_RC -ne 0 ]]; then
        echo "./convert.py returned with error $RUN_EXPERIMENT_CONVERT_RC converting: $result"
      fi
    done

    # Define function to call analysis so can call multiple times
    # https://github.com/llm-d/llm-d-benchmark/blob/main/analysis/vllm-benchmark-analyze_results.sh
    analyze_results () {
      mkdir -p $RESULTS_DIR/analysis
      result_start=$(grep -nr "Result ==" $RESULTS_DIR/stdout.log | cut -d ':' -f 1)
      total_file_lenght=$(cat $RESULTS_DIR/stdout.log | wc -l)
      cat $RESULTS_DIR/stdout.log | sed "$result_start,$total_file_lenght!d" > $RESULTS_DIR/analysis/summary.txt
      return $?
    }

    # https://github.com/llm-d/llm-d-benchmark/blob/main/build/llm-d-benchmark.sh#L63-L74
    echo "üîÑ Running analysis"
    # Try to run analysis twice then give up
    analyze_results
    ec=$?
    if [[ $ec -ne 0 ]]; then
      echo "execution of analyzer failed, wating 120 seconds and trying again"
      sleep 120
      set -x
      analyze_results
    fi
    # Return with error code of first iteration of experiment analyzer
    # TBD modify this message depending on success
    echo "‚úÖ Results analyzed and reports generated"
    exit $ec

