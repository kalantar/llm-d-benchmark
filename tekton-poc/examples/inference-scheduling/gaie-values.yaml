inferenceExtension:
  replicas: 1
  image:
    # Either image will work, you just need to bring the correct plugins per image. In this example we will bring the upstream default plugin
    ###################
    name: llm-d-inference-scheduler
    hub: ghcr.io/llm-d
    tag: v0.2.1
    pullPolicy: Always
  extProcPort: 9002
  extraContainerPorts:
    - name: zmq
      containerPort: 5557
      protocol: TCP
  extraServicePorts:
    - name: zmq
      port: 5557
      targetPort: 5557
      protocol: TCP
  env:
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-secret
          key: HF_TOKEN
  pluginsConfigFile: "inf-sche-load.yaml"
  pluginsCustomConfig:
    inf-sche-upstream-default.yaml: |
      # Sample EPP configuration for running without P/D with the default plugins
      # https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/123ad68c59aff1060a4022c394c52d16cd5d86b7/config/charts/inferencepool/templates/epp-config.yaml#L7
      apiVersion: inference.networking.x-k8s.io/v1alpha1
      kind: EndpointPickerConfig
      plugins:
      - type: decode-filter
      - type: queue-scorer
      - type: prefix-cache-scorer
      - type: single-profile-handler
      - type: max-score-picker
      schedulingProfiles:
      - name: default
        plugins:
        - pluginRef: decode-filter
        - pluginRef: queue-scorer
          weight: 1
        - pluginRef: prefix-cache-scorer
          weight: 1
    inf-sche-load.yaml: |
      # Sample EPP configuration for running without P/D with load-aware scorer
      apiVersion: inference.networking.x-k8s.io/v1alpha1
      kind: EndpointPickerConfig
      plugins:
      - type: decode-filter
      - type: max-score-picker
      - type: single-profile-handler
      - type: load-aware-scorer
      schedulingProfiles:
      - name: default
        plugins:
        - pluginRef: decode-filter
        - pluginRef: max-score-picker
        - pluginRef: load-aware-scorer
          weight: 1
inferencePool:
  targetPortNumber: 8000
  modelServerType: vllm
  apiVersion: "inference.networking.x-k8s.io/v1alpha2"
  modelServers:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: qwen-qwen3-0-6b
provider:
  name: none
