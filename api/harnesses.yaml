# work in progress
apiVersion: v1
kind: Pod
metadata:
  name: llmdbench-inference-perf-launcher
  labels:
    app: ${LLMDBENCH_RUN_HARNESS_LAUNCHER_NAME}
spec:
  serviceAccountName: $LLMDBENCH_HARNESS_SERVICE_ACCOUNT
  containers:
  - name: harness
    image: ghcr.io/llm-d/llm-d-benchmark:v0.3.0rc2
    imagePullPolicy: Always
    securityContext:
      runAsUser: 0
    command: ["sh", "-c"]
    args:
    - llm-d-benchmark.sh
    resources:
      limits:
        cpu: 16
        memory: 32Gi
      requests:
        cpu: 16
        memory: 32Gi
    env:
    - name: LLMDBENCH_RUN_EXPERIMENT_LAUNCHER
      value: "1"
    - name: LLMDBENCH_RUN_EXPERIMENT_ANALYZE_LOCALLY
      value: "0"
    # the model endpoint
    - name: LLMDBENCH_HARNESS_STACK_ENDPOINT_URL. # model url
      value: FILL_IN
    - name: LLMDBENCH_RUN_EXPERIMENT_ANALYZER. @hen analyzelocally is 1
      value: "inference-perf-analyze_results.sh"
    - name: LLMDBENCH_RUN_EXPERIMENT_HARNESS_WORKLOAD_NAME
      value: FILL_IN
    - name: LLMDBENCH_RUN_EXPERIMENT_ID
      value: FILL_IN
    - name: LLMDBENCH_HARNESS_NAME
      value: inference-perf
    - name: LLMDBENCH_RUN_EXPERIMENT_RESULTS_DIR
      value: FILL_IN
    - name: LLMDBENCH_CONTROL_WORK_DIR
      value: FILL_IN
    - name: LLMDBENCH_HARNESS_NAMESPACE
      value: FILL_IN
    - name: LLMDBENCH_HARNESS_STACK_TYPE
      value: FILL_IN   # vllm-prod, llm-d, mock   what used for???  fmperf needs it
    - name: LLMDBENCH_HARNESS_STACK_ENDPOINT_URL. # duplicate
      value: FILL_IN
    - name: LLMDBENCH_HARNESS_STACK_NAME  # label used for output file; 
      value: FILL_IN
    # does not seem to get used
    # - name: LLMDBENCH_DEPLOY_METHODS
    #   value: "${LLMDBENCH_DEPLOY_METHODS}"
    - name: LLMDBENCH_MAGIC_ENVAR
      value: "harness_pod"
    $(add_env_vars_to_pod $LLMDBENCH_CONTROL_ENV_VAR_LIST_TO_POD)
    - name: HF_TOKEN_SECRET
      value: hf-secret
    - name: HUGGING_FACE_HUB_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-secret
          key: HF_TOKEN
    volumeMounts:
    - name: results
      mountPath: /requests
    - name: inference-perf-profiles
      mountPath: /workspace/profiles/inference-perf
  volumes:
  - name: results
    persistentVolumeClaim:
      claimName: FILL_IN
  - name: inference-perf-profiles
    configMap:
      name: inference-perf-profiles
  restartPolicy: Never