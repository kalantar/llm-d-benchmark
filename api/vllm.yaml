---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: meta-lla-cee165fc-instruct
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.9
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/decode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: meta-lla-cee165fc-instruct-decode
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.9
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    deployed-by: kalantar
    modelservice: llm-d-benchmark
spec:
  replicas: 2
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: meta-lla-cee165fc-instruct
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: meta-lla-cee165fc-instruct
        llm-d.ai/role: decode
      annotations:
        deployed-by: kalantar
        modelservice: llm-d-benchmark
    spec:
      initContainers:
        - name: routing-proxy
          args:
            - --port=8000
            - --vllm-port=8200
            - --connector=nixlv2
            - -v=3
            - --secure-proxy=false
          image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.3.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
          resources: {}
          restartPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
    
      serviceAccountName: meta-lla-cee165fc-instruct
      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: nvidia.com/gpu.product
                  operator: In
                  values:
                    - NVIDIA-H100-80GB-HBM3
      volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 16Gi
          name: dshm
        - name: model-storage
          persistentVolumeClaim:
            claimName: model-pvc
            readOnly: true
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d:v0.2.0
            
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 5
            tcpSocket:
              port: 8200
          ports:
          - containerPort: 5557
            protocol: TCP
          - containerPort: 8200
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8200
            periodSeconds: 5
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8200
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
          
          command: ["vllm", "serve"]
          args:
            - /model-cache/models/meta-llama/Llama-3.1-8B-Instruct
            - --port
            - "8200"
            - --served-model-name
            - "meta-llama/Llama-3.1-8B-Instruct"
            
            - --enforce-eager
            - --block-size
            - "64"
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
            - --disable-log-requests
            - --disable-uvicorn-access-log
            - --max-model-len
            - "16000"
            - --tensor-parallel-size
            - "4"
          env:
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: UCX_TLS
            value: cuda_ipc,cuda_copy,tcp
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5557"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: NCCL_DEBUG
            value: INFO
          - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
            value: "1"
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "4"
          
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: llm-d-hf-token
                key: HF_TOKEN
          
          resources:
            limits:
              cpu: "16"
              memory: 64Gi
              nvidia.com/gpu: "4"
            requests:
              cpu: "16"
              memory: 64Gi
              nvidia.com/gpu: "4"
          
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            - name: model-storage
              mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/httproute.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: meta-lla-cee165fc-instruct
  namespace: llmdbench
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.9
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-install,post-upgrade
spec:
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: infra-llmdbench-inference-gateway
  rules:
  - backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: meta-lla-cee165fc-instruct-gaie
      port: 8000
      weight: 1
    filters:
    - type: URLRewrite
      urlRewrite:
        path:
          replacePrefixMatch: /
          type: ReplacePrefixMatch
    matches:
    - path:
        type: PathPrefix
        value: /meta-llama-llama-3-1-8b-instruct/
    timeouts:
      backendRequest: 0s
      request: 0s
  - backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: meta-lla-cee165fc-instruct-gaie
      port: 8000
      weight: 1
    timeouts:
      backendRequest: 0s
      request: 0s
